{"cells":[{"cell_type":"markdown","metadata":{"id":"CFGHU5ZRDUcS"},"source":["<img src=\"../images/cads-logo.png\" style=\"height: 100px;\" align=left>\n","<img src=\"../images/sklearn-logo.png\" style=\"height: 100px;\" align=right>"]},{"cell_type":"markdown","metadata":{"id":"ARgG5BifDUcU"},"source":["# Classification\n","\n","In classification problems, the task is to decide to which of a predefined, finite set of categories an object belongs."]},{"cell_type":"markdown","metadata":{"id":"vASkLrOCDUcV"},"source":["# Table of Contents\n","\n","- [1. k-Nearest Neighbours: Algorithm](#1.-k-Nearest-Neighbours:-Algorithm)\n","    - [How sensitive is k-NN classification accuracy to the choice of the 'k' parameter?](#How-sensitive-is-k-NN-classification-accuracy-to-the-choice-of-the-'k'-parameter?)\n","    - [How sensitive is k-NN classification accuracy to the train/test split proportion?](#How-sensitive-is-k-NN-classification-accuracy-to-the-train/test-split-proportion?)\n","    - [KNN and Distance Functions](#KNN-and-Distance-Functions)\n","        - [Distance Functions:](#Distance-Functions:)\n","- [2. Decision Trees: Classification](#2.-Decision-Trees:-Classification)\n","    - [Decision trees and over-fitting](#Decision-trees-and-over-fitting)\n","- [3. Logistic Regression: Classification](#3.-Logistic-Regression:-Classification)\n","    - [Logistic Polynomial Regression](#Logistic-Polynomial-Regression)\n","    - [Regularization in Logistic Regression](#Regularization-in-Logistic-Regression)\n","- [4. Support Vector Machine (SVM): Classification](#4.-Support-Vector-Machine-(SVM):-Classification)\n","    - [Motivating SVMs](#Motivating-SVMs)\n","    - [Maximizing the margin:](#Maximizing-the-margin:)\n","    - [Tuning the SVM: Softening the margin](#Tuning-the-SVM:-Softening-the-margin)\n","        - [Non-linear kernels](#Non-linear-kernels)\n","        - [Gaussian Radial Basis Function](#Gaussian-Radial-Basis-Function)\n","        - [Other kernels](#Other-kernels)\n","- [More Readings:](#More-Readings:)"]},{"cell_type":"markdown","metadata":{"id":"r6SarwqiDUcX"},"source":["# 1. k-Nearest Neighbours: Algorithm\n","\n","\n","``k-Nearest Neighbours (KNN)`` falls in the **supervised learning** family of algorithms and it is a famous classification algorithm because it is easy to understand and its good performance however, it is computationally expensive. The KNN algorithm classifies an unlabelled test sample based on the majority of similar samples among the k number of its nearest neighbors.\n","\n","\n","## Algorithm\n","\n"," <img src='../images/knn2.jpg'/>\n","\n"," The basic KNN classifier steps can be described as follows:\n","- 1. **Training phase:** The training samples and the class labels of these samples are stored. no missing data\n","       allowed. No non-numeric features allowed in case of using Euclidian distance. In case of categorical\n","       features new distance function should be defined.<br><br>\n","- 2. **Classification phase:** Each test sample is classified using majority vote of its neighbors by the following\n","       steps: <br>\n","       a) Distances from the test sample to all stored training sample are calculated using a specific distance function or similarity measure. <br><br>\n","       b) The K nearest neighbors of the test sample are selected, where K is a pre-defined small integer. <br><br>\n","       c) The most repeated class of these K neighbors is assigned to the test sample. In other words, a test sample is assigned to the class c if it is the most frequent class label among the K nearest training samples."]},{"cell_type":"markdown","metadata":{"id":"39MXyiYdDUcX"},"source":["**Import required modules and load data file:**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fAm6YQb0DUcY"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import pandas as pd\n","from sklearn.model_selection import train_test_split"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cCnh7hJrDUcZ"},"outputs":[],"source":["# Load dataset\n","names = ['sepal-length', 'sepal-width', 'petal-length', 'petal-width', 'class']\n","dataset = pd.read_csv('../data/iris.csv', names=names, skiprows=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vpzVaz0KDUcZ"},"outputs":[],"source":["dataset.info()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nG76RguyDUca"},"outputs":[],"source":["dataset.head(10)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Xv0RwOXrDUcb"},"outputs":[],"source":["# class distribution\n","print(dataset.groupby('class').size())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cvwPX5SEDUcb"},"outputs":[],"source":["# box and whisker plots\n","dataset.plot(kind='box', subplots=True, layout=(2,2), sharex=False, sharey=False)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Sex7BEQ7DUcc"},"outputs":[],"source":["# histograms\n","dataset.hist()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3uFC-DYlDUcc"},"outputs":[],"source":["sns.pairplot(dataset, kind=\"scatter\", hue=\"class\", markers=[\"o\", \"s\", \"D\"], palette=\"Set2\")\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"5psSl8wqDUcd"},"source":["**Create X and y**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-f0lti00DUcd"},"outputs":[],"source":["X = dataset.drop('class', axis=1)\n","y = dataset['class']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VA8qlnTZDUcd"},"outputs":[],"source":["X.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nfTml9vxDUcd"},"outputs":[],"source":["y.head()"]},{"cell_type":"markdown","metadata":{"id":"Sz5WK_uRDUce"},"source":["**Split data train_test:**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6J_0ydAVDUce"},"outputs":[],"source":["# default is 75% / 25% train-test split\n","X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=7)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5mtUudfYDUcf"},"outputs":[],"source":["X_train.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YS29PjAnDUcf"},"outputs":[],"source":["y_train.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"q6Xqcft9DUcf"},"outputs":[],"source":["print(X_train.shape)\n","print(y_train.shape)\n","print(X_test.shape)\n","print(y_test.shape)"]},{"cell_type":"markdown","metadata":{"id":"dNj8izDuDUcg"},"source":["**Create classifier object:**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MddBnhNsDUcg"},"outputs":[],"source":["from sklearn.neighbors import KNeighborsClassifier\n","\n","knn = KNeighborsClassifier(n_neighbors=5)"]},{"cell_type":"markdown","metadata":{"id":"6N-icxkeDUcg"},"source":["**Train the classifier (fit the estimator) using the training data:**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mEiTXBwXDUcg"},"outputs":[],"source":["knn.fit(X_train, y_train)"]},{"cell_type":"markdown","metadata":{"id":"NJ8-i1vfDUcg"},"source":["**Use the trained k-NN classifier model to classify new, previously unseen objects:**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vRYouucDDUch"},"outputs":[],"source":["# first example: a small flower with sepal-length=2,\n","#            sepal-width=3.2, petal-length=1.5, petal-width=0.5\n","species_prediction = knn.predict([[2, 3.2, 1.5, 0.5]])\n","print('species name is '+ species_prediction[0])\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZbhXBsXsDUch"},"outputs":[],"source":["# first example: a small flower with sepal-length=2,\n","#            sepal-width=3.2, petal-length=1.5, petal-width=0.5\n","species_prediction = knn.predict([[2, 3.2, 1.5, 0.5]])\n","print('species name is '+ species_prediction[0])\n"]},{"cell_type":"markdown","metadata":{"id":"I7ZEa6MGDUch"},"source":["**Estimate the accuracy of the classifier on future data, using the test data:**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wN4uE2nvDUch"},"outputs":[],"source":["from sklearn.metrics import accuracy_score\n","\n","y_pred = knn.predict(X_test)\n","print('KNN Accuracy = {:.2f}'.format(accuracy_score(y_test, y_pred)))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"U4psUVLIDUci"},"outputs":[],"source":["#accuracy\n","knn.score(X_test, y_test), knn.score(X_train, y_train)"]},{"cell_type":"markdown","metadata":{"id":"zOTzWGiZDUci"},"source":["### How sensitive is k-NN classification accuracy to the choice of the 'k' parameter?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pqRvnWdYDUci"},"outputs":[],"source":["k_range = range(1,20)\n","scores = []\n","\n","for k in k_range:\n","    knn = KNeighborsClassifier(n_neighbors = k, weights='uniform')\n","    knn.fit(X_train, y_train)\n","    scores.append(knn.score(X_test, y_test))\n","\n","plt.figure()\n","plt.xlabel('k')\n","plt.ylabel('accuracy')\n","plt.title('Accuracy by n_neigbors')\n","plt.scatter(k_range, scores)\n","plt.xticks([0,5,10,15,20]);"]},{"cell_type":"markdown","metadata":{"id":"eDrY59BRDUci"},"source":["### How sensitive is k-NN classification accuracy to the train/test split proportion?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"el7aPWoUDUci"},"outputs":[],"source":["# size of the training set\n","t = [0.8, 0.7, 0.6, 0.5, 0.4, 0.3, 0.2]\n","\n","knn = KNeighborsClassifier(n_neighbors = 5)\n","\n","plt.figure()\n","\n","scores_test_mean = []\n","scores_train_mean = []\n","\n","for s in t:\n","    scores_test = []\n","    scores_train = []\n","    for i in range(1,1000):\n","        X_train, X_test, y_train, y_test = train_test_split(X,\n","                                                            y,\n","                                                            test_size=1-s)\n","        knn.fit(X_train, y_train)\n","        scores_test.append(knn.score(X_test, y_test))\n","        scores_train.append(knn.score(X_train, y_train))\n","    scores_test_mean.append(np.mean(scores_test))\n","    scores_train_mean.append(np.mean(scores_train))\n","\n","plt.plot(t, scores_test_mean, color='red', label='Accuracy on Test')\n","plt.plot(t, scores_train_mean, color='blue', label='Accuracy on Train')\n","\n","plt.xlabel('Training set proportion (%)')\n","plt.ylabel('Accuracy')\n","plt.legend()\n","plt.title('Accuracy by Split_size')"]},{"cell_type":"markdown","metadata":{"id":"PNyQ5LgODUcj"},"source":["### KNN and Distance Functions\n","\n","KNeighborsClassifier can receive hyperparameters such as:\n","\n","- n_neighbors : int, optional (default = 5)\n","    Number of neighbors to use.\n","    \n","- p : integer, optional (default = 2)\n","    Power parameter for the Minkowski metric. When p = 1, this is\n","    equivalent to using manhattan_distance (l1), and euclidean_distance\n","    (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.\n","\n","- metric : string or callable, default 'minkowski'\n","    the distance metric to use for the tree.  The default metric is\n","    minkowski, and with p=2 is equivalent to the standard Euclidean\n","    metric. See the documentation of the DistanceMetric class for a\n","    list of available metrics.\n","    \n","To view to list of all the hyperparameters for KNeighborsClassifier you can run the command \"?KNeighborsClassifier\" These parameters help the user to change the distance function in KNN algorithm."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Cds-gI8nDUcj"},"outputs":[],"source":["# ?KNeighborsClassifier"]},{"cell_type":"markdown","metadata":{"id":"JvLPzdrhDUcj"},"source":["#### Distance Functions:\n","\n","The Minkowski distance of order $p$ between two points $A=(a_1, a_2, ..., a_n) \\in \\mathbb{R}^n$ and $B=(b_1, b_2, ..., b_n) \\in \\mathbb{R}^n$ is defined as:$$ d_p(A,B)= \\left(\\sum_{i=1}^n |a_i-b_i|^p\\right)^{1/p} $$\n","\n"," <img src='../images/minko.png' style=\"height: 350px;\">\n","\n","To simplify the problem assume there are two datapoints A=[1, 1] and B=[5, 4]. Compute the distance between these two points according to the following definitions of the distance function for p=1 (Manhattan Distance) and p=2 (Euclidian Distance):"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sM2228LfDUcj"},"outputs":[],"source":["import math\n","A=np.array([1, 1])\n","B=np.array([5, 4])\n","\n","# Euclidian\n","d=math.sqrt((A[0]-B[0])**2+(A[1]-B[1])**2)\n","print('Euclidian distance between A and B is: ', d)\n","\n","# Manhatan\n","d = abs((A[0]-B[0]))+ abs((A[1]-B[1]))\n","print('Manhatan distance between A and B is: ', d)"]},{"cell_type":"markdown","metadata":{"id":"WTq_8jHnDUck"},"source":["**Exercise:**\n","Try to classify Iris data using KNN for n_neighbors=7 using both Euclidian and Manhatan distances. Name these models as \"E_knn\" and \"M_knn\". Compare their accuracy scores. Which model is better?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RutQDX4UDUck"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"I5iS3vtsDUck"},"source":["### Pros and Cons of KNN\n","\n","**Pros**\n","\n","As you can already tell from the previous section, one of the most attractive features of the K-nearest neighbor algorithm is that is **simple to understand** and **easy to implement**. Furthermore, KNN works just as easily with **multiclass datasets** whereas other algorithms are hardcoded for the binary setting. Finally, as we mentioned earlier, the **non-parametric nature** of KNN gives it an edge in certain settings where the data may be highly “unusual”.\n","\n","\n","**Cons**\n","\n","One of the obvious drawbacks of the KNN algorithm is the **computationally expensive** testing phase which is impractical in industry settings. Furthermore, KNN can **suffer from skewed class distributions**. For example, if a certain class is very frequent in the training set, it will tend to dominate the majority voting of the new example (large number = more common). Finally, the accuracy of KNN can be severely degraded with **high-dimension data** because there is little difference between the nearest and farthest neighbor.\n","\n","### Improvements\n","\n","With that being said, there are many ways in which the KNN algorithm can be improved.\n","\n","- A simple and effective way to remedy skewed class distributions is by implementing **weighted voting**. The class of each of the K neighbors is multiplied by a weight proportional to the inverse of the distance from that point to the given test point. This ensures that nearer neighbors contribute more to the final vote than the more distant ones. To use weighted voting, in `KNeighborsClassifier()` set `weights = 'distance'`.\n","\n"," <img src='../images/WeightedKnn.png'/>\n","\n","\n","- **Changing the distance metric** for different applications may help improve the accuracy of the algorithm. (i.e. Hamming distance for text classification)\n","- **Rescaling your data** makes the distance metric more meaningful. For instance, given 2 features $height$ and $weight$, an observation such as $x=[180,7]$ will clearly skew the distance metric in favor of height. One way of fixing this is by column-wise subtracting the mean and dividing by the standard deviation. Scikit-learn’s normalize() method can come in handy.\n","- **Dimensionality reduction** techniques like PCA should be executed prior to appplying KNN and help make the distance metric more meaningful.\n","- **Approximate Nearest Neighbor** techniques such as using k-d trees to store the training observations can be leveraged to decrease testing time. Note however that these methods tend to perform poorly in high dimensions (20+)."]},{"cell_type":"markdown","metadata":{"id":"6VeYOjmUDUck"},"source":["## Exercise\n","\n","Load the fruits data file and apply KNN to classify fruits.\n","\n","1. Load Data\n","2. Examine data\n","3. Create train-test split\n","4. Create classifier object\n","5. Train the classifier (fit the estimator) using the training data for k=5\n","6. Estimate the accuracy of the classifier on future data, using the test data\n","7. Compare the accuracy of your model for different values of k on train and test\n","8. Compare the accuracy of your model for different distance metrics on train and test\n","9. Examine how scaling the data will impact the accuracy"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uloTJVyzDUck"},"outputs":[],"source":["fruits = pd.read_table('../Data/fruit_data_with_colors.txt')\n","fruits.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0DZX-rZBDUcl"},"outputs":[],"source":["fruits.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"X-yzYXk1DUcl"},"outputs":[],"source":["#  - 2. Examine data\n","# your code here"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ypVl7I24DUcl"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Se0KM9aQDUcl"},"outputs":[],"source":["X = fruits[['mass', 'width', 'height','color_score']]\n","y = fruits['fruit_label']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gqKxnP5uDUcl"},"outputs":[],"source":["X.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6KE8opY1DUcm"},"outputs":[],"source":["y.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mUIoIJ4RDUcm"},"outputs":[],"source":["#  - 3. Create train-test split\n","# your code here"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OE1CrI5QDUcm"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Otc2FwFBDUcn"},"outputs":[],"source":["# your code here"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AMF6BvrLDUcn"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"p1dUooytDUcn"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f4Lks3KtDUcn"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_qKSwSKmDUcn"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"sCNtUWBVDUcn"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"w-2ORxd1DUco"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XvOgabivDUco"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"collapsed":true,"id":"jka7jczXDUco"},"source":["# 2. Decision Trees: Classification\n","\n","Decision trees are widely used models for classification and regressions. Essentially, they learn a hierarchy of if/else questions, leading to an action.  \n","\n","These questions are similar to the questions you might ask in a **game of 20 Questions**. Imagine you want to distinguish between the following four animals:\n","\n","**bears, hawks, penguins, and dolphins**.\n","\n","<img src='../images/DecisionTree.png'>\n","\n","Your goal is to get to the right answer by asking as few if/else questions as possible. You might start off by asking whether the animal has feathers, a question that narrows down your possible animals to just two. If the answer is \"yes\", you can ask another question that could help you distinguish between hawks and penguins. For example, you could ask whether the animal can fly. If the animal doesn’t have feathers, your possible animal choices are dolphins and bears, and you will need to ask a question to distinguish between these two animals—for example, asking whether the animal has finns. This series of questions can be expressed as a decision tree.\n","\n","Machine learning speaking, we want to build a model to distinguish between these four classes of animals using three features (has feathers, can fly, has fins). But instead of building these models by hand, we can learn them from data using supervised learning.\n","\n","### Building decision trees\n","\n","Usually, data does not come in the form of binary yes/no features, but is instead represented by continuous features. So, we expect the \"questions\" to be in \"threshold\"-like form, such as \"Is feature *i* larger than value *a*?\" To build a tree, the algorithm has to search over all possible conditions and finds the one that is most informative about the target variable. Using the two moons toy dataset, let's see how it's done.\n","\n","![](../images/decisiontrees-step1.png)\n","\n","First, splitting the dataset vertically at x[1]=0.0596 yields the most information; it best separates the points in class 1 from the points in class 2. The top node, also called the root, represents the whole dataset, consisting of 75 points belonging to class 0 and 75 points belonging to class 1. The split is done by testing whether x[1] <= 0.0596, indicated by a black line. If the test is true, a point is assigned to the left node, which contains 2 points belonging to class 0 and 32 points belonging to class 1. Otherwise the point is assigned to the right node, which contains 48 points belonging to class 0 and 18 points belonging to class 1. Even though the first split did a good job of separating the two classes, the bottom region still contains points belonging to class 0, and the top region still contains points belonging to class 1.\n","\n","![](../images/decisiontrees-step2.png)\n","\n","Next, we repeat the process of looking for the best (or the most informative) way to split further in both regions. This is depth 2 of the decision tree. This recursive process yields a binary tree of decisions, with each node containing a\n","test condition. We can view this algorithm as building a hierarchical partition. The recursive partitioning of the data is repeated until each region in the partition (each leaf in the decision tree) only contains a single target value (a single class or a single regression value). A leaf of the tree that contains data points that all share the same target value is called *pure*. The final partitioning for this dataset is as follows:\n","\n","![](../images/decisiontrees-finalstep.png)\n","\n","**Predicting the target class or value of new data point:**\n","\n","A prediction on a new data point is made by checking which region of the partition of the feature space the point lies in, and then predicting the target class with the majority (or the single target in the case of *pure* leaves) in that region. The region can be found by traversing the tree from the root and going left or right depending on whether the test condition is fulfilled or not.\n","\n","It is also possible to use decision trees for regression tasks, using exactly the same technique. To make a prediction, we traverse the tree based on the tests in each node and find the leaf the new data point falls into. The output for this data point is the mean target value of the training points in this leaf."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s3T4aGW_DUco"},"outputs":[],"source":["# Load dataset\n","names = ['sepal-length', 'sepal-width', 'petal-length', 'petal-width', 'class']\n","dataset = pd.read_csv('../data/iris.csv', names=names, skiprows=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MyeCLNalDUcp"},"outputs":[],"source":["X = dataset.drop('class', axis=1)\n","y = dataset['class']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GaqvCNk6DUcp"},"outputs":[],"source":["X.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"T182ehLvDUcp"},"outputs":[],"source":["y.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DittwozwDUcp"},"outputs":[],"source":["from sklearn.tree import DecisionTreeClassifier\n","X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=7)\n","\n","tree = DecisionTreeClassifier(max_depth=2)\n","tree.fit(X_train, y_train)\n","\n","print(\"Accuracy on training set: {:.3f}\".format(tree.score(X_train, y_train)))\n","print(\"Accuracy on test set: {:.3f}\".format(tree.score(X_test, y_test)))"]},{"cell_type":"markdown","metadata":{"id":"6n_QDi-yDUcp"},"source":["Question: Find out what's the ideal depth level that we can strike a balance between fitting well to the training data, and generalizing well to test data."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c4aAyCjyDUcp"},"outputs":[],"source":["train_acc = []\n","test_acc = []\n","\n","for i in range(2,10):\n","    tree1 = DecisionTreeClassifier(max_depth=i, random_state=0)\n","    tree1.fit(X_train, y_train)\n","\n","    train_acc.append(tree1.score(X_train, y_train))\n","    test_acc.append(tree1.score(X_test, y_test))\n","\n","plt.plot(range (2,10),train_acc,'b-', label='Train Accuracy')\n","plt.plot(range (2,10),test_acc,'r-', label='Test Accuracy')\n","plt.xlabel('Max_Depth')\n","plt.ylabel('Accuracy')\n","plt.legend()\n","plt.title('Accuracy by Depth of DecisionTree')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"EUTUGcW5DUcp"},"source":["**Exercise:**\n","Create, fit, and evaluate a desicion tree classifier on our dataset for max_depth=3. Compare it with the previous model \"tree\"."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ap5842uxDUcp"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"98inb77GDUcp"},"source":["### Decision trees and over-fitting\n","\n","Such over-fitting turns out to be a general property of decision trees: it is very easy to go too deep in the tree, and thus to fit details of the particular data rather than the overall properties of the distributions they are drawn from.\n","Another way to see this over-fitting is to look at models trained on different subsets of the data—for example, in this figure we train two different trees, each on half of the original data:"]},{"cell_type":"markdown","metadata":{"id":"asSaT5weDUcr"},"source":["Image:\n","![](../images/decision-tree-overfitting.png)\n"]},{"cell_type":"markdown","metadata":{"id":"hRBZ_PohDUcr"},"source":["It is clear that in some places, the two trees produce consistent results (e.g., in the four corners), while in other places, the two trees give very different classifications (e.g., in the regions between any two clusters).\n","The key observation is that the inconsistencies tend to happen where the classification is less certain, and thus by using information from *both* of these trees, we might come up with a better result!"]},{"cell_type":"markdown","metadata":{"id":"tB9ErzRtDUcr"},"source":["Just as using information from two trees improves our results, we might expect that using information from many trees would improve our results even further."]},{"cell_type":"markdown","metadata":{"id":"Z9I5bghEDUcr"},"source":["# 3. Logistic Regression: Classification\n","\n","## Can we use Linear regression to predict a class?\n","\n","In a classification problem the values $y$ that we want to predict take on only a small number of discrete values\n","\n","### Binary Classification:\n","\n","For now, we will focus on the binary classification problem in which $y$\n","can take on only two values, 0 and 1.\n","\n","We will see later how to use a binary classifiers for the multiple-class case.\n","\n","For instance, if we are trying to build a spam classifier for email, then $x$ represents a mail (frequency of each word for instance), and $y$ may be 1 if it is a piece of spam mail, and 0 otherwise. 0 is also called the\n","negative class, and 1 the positive class, and they are sometimes also denoted by the symbols “-”\n","and “+”.\n","\n","\n","### Linear Regression limitation:\n","\n","In linear regression, the prediction is given by\n","\n","$$\\hat{y} = \\beta_0 + \\beta_1 x_1 + \\cdots + \\beta_m x_m$$\n","\n","$$\\hat{y} = \\sum_{j=0}^{m} \\beta_i x_i$$\n","\n","Then $\\hat{y}$ takes values in $\\mathbb{R}$.\n","\n","This is not what we want because in binary classification we have: $y \\in \\{0, 1\\}$\n","\n","### The Sigmoid function:\n","\n","If we still want to use a linear combination of the input features (the $x_i$'s) to predict $y$, we need a function that takes input in $\\mathbb{R}$ and returns values in $[0, 1]$.\n","\n","The sigmoid function:\n","$$\n","\\begin{align*}\n","  \\phi \\colon \\mathbb{R} & \\to ]0, 1[\\\\\n","  z &\\mapsto \\frac{1}{1+e^{-z}}\n","\\end{align*}\n","$$"]},{"cell_type":"markdown","metadata":{"id":"_XJvIVM8DUcr"},"source":["### Exercise\n","\n","1. Plot the function $\\phi(z)$ for $z\\in [-7, 7]$."]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"GagNrGasDUcr"},"outputs":[],"source":["a=np.arange(-7, 7, 0.01)\n","b=1/(1+np.exp(-a))\n","plt.plot(a,b, lw=2)\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"dsXXboxNDUcr"},"source":["2. What are the value of\n","    - $\\phi(0)$\n","    - $\\lim_{z\\to+\\infty}\\phi(z)$\n","    - $\\lim_{z\\to-\\infty}\\phi(z)$"]},{"cell_type":"markdown","metadata":{"id":"BTvYVcnoDUcr"},"source":["Therefore, if for x as a datapoint instead of $\\hat{y}$ we calsulate $\\phi(\\hat{y})$ this will be the estimate probability of y=1 for x. For example, if for x, $\\phi(\\hat{y})=\\sum_{j=0}^{m} \\beta_i x_i=0.65$ we interpret that the probability of x being in class 1 is 0.65."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ayRv2w7qDUcr"},"outputs":[],"source":["# Load dataset\n","names = ['sepal-length', 'sepal-width', 'petal-length', 'petal-width', 'class']\n","dataset = pd.read_csv('../data/iris.csv', names=names, skiprows=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dK2M8g-wDUcr"},"outputs":[],"source":["X = dataset.drop('class', axis=1)\n","y = dataset['class']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xiXhL7j9DUcr"},"outputs":[],"source":["y.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sPfJyE9xDUcr"},"outputs":[],"source":["X.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OBqX_2HcDUcs"},"outputs":[],"source":["# Make out dataset as binary class\n","n_X = X.iloc[:, :2]\n","n_X.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qaNTrTSuDUcs"},"outputs":[],"source":["n_X.loc[:,'class'] = pd.Series(np.where(dataset['class']=='setosa', 1,0))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yTSrcsmfDUcs"},"outputs":[],"source":["n_X.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2bMS2ONtDUct"},"outputs":[],"source":["n_y = n_X.loc[:,'class']\n","n_X = n_X.drop('class', axis=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6URZbXF7DUct"},"outputs":[],"source":["n_X_train, n_X_test, n_y_train, n_y_test = train_test_split(n_X, n_y,\n","                                                            test_size = .2,\n","                                                            random_state=7)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DIjmASxHDUct"},"outputs":[],"source":["n_y_train.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iVyhhOYADUct"},"outputs":[],"source":["from sklearn.linear_model import LogisticRegression\n","logr = LogisticRegression()\n","logr.fit(n_X_train, n_y_train)\n","logr_y_pred = logr.predict(n_X_test)\n","print('Accuracy: ',logr.score(n_X_test, n_y_test))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jcnDuhYwDUct"},"outputs":[],"source":["beta0 = logr.intercept_[0]\n","beta1 = logr.coef_[0][0]\n","beta2 = logr.coef_[0][1]\n","\n","print('beta0 = {}, beta1 = {}, beta2 = {} '.format(beta0, beta1, beta2))"]},{"cell_type":"raw","metadata":{"id":"LY2S8M8_DUct"},"source":["beta0+beta1*x[0]+beta2*X[1]=0\n","beta2*X[1]=-beta0-beta1*x[0]\n","X[1]=-beta0/beta2-(beta1/beta2)*x[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":false,"id":"H3HKryf2DUcu"},"outputs":[],"source":["# actuals\n","\n","x1 = np.linspace(min(n_X_train.iloc[:,0])-1,max(n_X_train.iloc[:,0])+1, 50)\n","\n","print(\"Actuals\")\n","plt.plot(x1, -beta0/beta2 - (beta1/beta2) *x1) # decision boundary\n","plt.scatter(n_X_train.iloc[:,0], n_X_train.iloc[:,1], c=n_y_train)\n","plt.show();\n","\n","# predicted\n","print(\"Predicted\")\n","logr_y_pred_train = logr.predict(n_X_train)\n","\n","plt.plot(x1, -beta0/beta2 - (beta1/beta2) *x1) # decision boundary\n","plt.scatter(n_X_train.iloc[:,0], n_X_train.iloc[:,1], c=logr_y_pred_train)\n","plt.show();"]},{"cell_type":"markdown","metadata":{"id":"YMuPScl3DUcu"},"source":["## Logistic Polynomial Regression\n","\n","**Example:**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DSjFCXwEDUcu"},"outputs":[],"source":["def generate_data(n=50, a=10):\n","    rng = np.random.RandomState(1)\n","    X = 10*rng.rand(n, 2)\n","    Y = np.zeros(n)\n","    for i, x in enumerate(X):\n","        if (x[0]-5)**2 + (x[1]-4)**2 + 4*rng.rand()< 16:\n","            Y[i] = 1\n","    return X, Y\n","\n","print(\"Yellow is Class 1\")\n","print(\"Purple is Class 0\")\n","\n","X, Y = generate_data(300, 5)\n","plt.scatter(X[:,0], X[:,1], c=Y)\n","plt.show();"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"K7IQ-fdODUcu"},"outputs":[],"source":["print(\"X:\", X.shape)\n","print(\"Y:\", Y.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Blh_o2pfDUcv"},"outputs":[],"source":["model = LogisticRegression(fit_intercept=True, solver = 'liblinear')\n","model = model.fit(X, Y)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fEToSKEJDUcw"},"outputs":[],"source":["beta0 = model.intercept_[0]\n","beta1 = model.coef_[0][0]\n","beta2 = model.coef_[0][1]\n","\n","x1 = np.linspace(0, 10)\n","\n","# actuals\n","print(\"Actuals\")\n","plt.plot(x1, -beta0/beta2 - beta1/beta2 *x1) # decision boundary\n","plt.scatter(X[:,0], X[:,1], c=Y)\n","plt.show();\n","\n","# predicted\n","print(\"Predicted\")\n","ypred = model.predict(X)\n","plt.plot(x1, -beta0/beta2 - beta1/beta2 *x1) # decision boundary\n","plt.scatter(X[:,0], X[:,1], c=ypred)\n","plt.show();"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FXdzkpkqDUcx"},"outputs":[],"source":["from sklearn.preprocessing import PolynomialFeatures\n","\n","#[1, a, b, a^2, ab, b^2]\n","poly = PolynomialFeatures(degree=2, include_bias=False)\n","poly.fit(X)\n","x_poly = poly.transform(X)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UDfgstCWDUcx"},"outputs":[],"source":["X[1]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NSabzlL_DUcx"},"outputs":[],"source":["x_poly[1]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TmTdB9urDUcx"},"outputs":[],"source":["model_poly = LogisticRegression(fit_intercept=True, solver = 'liblinear')\n","model_poly = model_poly.fit(x_poly, Y)\n","print('accuracy = {:.2f}'.format(model_poly.score(x_poly, Y)))\n","\n","\n","# Decision boundry: beta0 + beta1 * x1 + beta2 * x2 + beta3 * x1^2 + beta4 * x1 * x2 + beta5 * x2^2\n","beta0_poly = model_poly.intercept_[0]\n","beta1_poly = model_poly.coef_[0][0]\n","beta2_poly = model_poly.coef_[0][1]\n","beta3_poly = model_poly.coef_[0][2]\n","beta4_poly = model_poly.coef_[0][3]\n","beta5_poly = model_poly.coef_[0][4]\n","print(beta0_poly, beta1_poly, beta2_poly,\n","      beta3_poly, beta4_poly, beta5_poly)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ogXVrcDIDUcy"},"outputs":[],"source":["# actuals\n","print(\"Actuals\")\n","plt.scatter(X[:,0], X[:,1], c=Y)\n","plt.show();\n","\n","# predicted\n","print(\"Predicted\")\n","ypred = model_poly.predict(x_poly)\n","\n","def f(x1,x2):\n","    return beta0_poly + beta1_poly * x1 + beta2_poly * x2 + beta3_poly * (x1**2) + beta4_poly * (x1 * x2) + beta5_poly * (x2**2)\n","\n","plt.tricontour(X[:,0], X[:,1], f(X[:,0], X[:,1]), levels = 0)\n","\n","plt.scatter(X[:,0], X[:,1], c = ypred)\n","\n","plt.show();"]},{"cell_type":"markdown","metadata":{"id":"F4WA_S5NDUcy"},"source":["## Regularization in Logistic Regression\n","\n","**Example:**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"y0TeWTGsDUcy"},"outputs":[],"source":["model_poly = LogisticRegression(fit_intercept=True,\n","                                solver = 'liblinear',\n","                                penalty = 'l1',\n","                                C = 1)\n","model_poly = model_poly.fit(x_poly, Y)\n","\n","print('accuracy = {:.2f}'.format(model_poly.score(x_poly, Y)))\n","\n","# Decision boundry: beta0 + beta1 * x1 + beta2 * x2 + beta3 * x1^2 + beta4 * x1 * x2 + beta5 * x2^2\n","beta0_poly = model_poly.intercept_[0]\n","beta1_poly = model_poly.coef_[0][0]\n","beta2_poly = model_poly.coef_[0][1]\n","beta3_poly = model_poly.coef_[0][2]\n","beta4_poly = model_poly.coef_[0][3]\n","beta5_poly = model_poly.coef_[0][4]\n","print(beta0_poly, beta1_poly, beta2_poly, beta3_poly, beta4_poly, beta5_poly)\n","\n","# actuals\n","print(\"Actuals\")\n","plt.scatter(X[:,0], X[:,1], c=Y)\n","plt.show();\n","\n","# predicted\n","print(\"Predicted\")\n","ypred = model_poly.predict(x_poly)\n","\n","def f(x1,x2):\n","    return beta0_poly + beta1_poly * x1 + beta2_poly * x2 + beta3_poly * (x1**2) + beta4_poly * (x1 * x2) + beta5_poly * (x2**2)\n","\n","plt.tricontour(X[:,0], X[:,1], f(X[:,0], X[:,1]), levels = 0)\n","\n","\n","plt.scatter(X[:,0], X[:,1], c = ypred)\n","\n","plt.show();"]},{"cell_type":"markdown","metadata":{"id":"8QleVXyrDUcy"},"source":["**Exercise:**\n","\n","Explore the impact of hyperparameters, `C` and `penalty` on the decision boundry and accuracy of the above Logistic Regression model."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"F9F5hVfwDUcy"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vDufcGOsDUcy"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XD-Q0wCoDUcy"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"I9CzmcY6DUcy"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"lO6ehHBMDUcz"},"source":["## Exercise - breast_cancer\n","For this exercise, you will be using the Breast Cancer Wisconsin (Diagnostic) Database that is available from sklearn datasets to create a  logistic regression classifier to predict the cancer status.\n","\n","**breast_cancer dataset columns name:**\n","\n","    ['mean radius', 'mean texture', 'mean perimeter', 'mean area',\n","    'mean smoothness', 'mean compactness', 'mean concavity',\n","    'mean concave points', 'mean symmetry', 'mean fractal dimension',\n","    'radius error', 'texture error', 'perimeter error', 'area error',\n","    'smoothness error', 'compactness error', 'concavity error',\n","    'concave points error', 'symmetry error', 'fractal dimension error',\n","    'worst radius', 'worst texture', 'worst perimeter', 'worst area',\n","    'worst smoothness', 'worst compactness', 'worst concavity',\n","    'worst concave points', 'worst symmetry', 'worst fractal dimension',\n","    'target']\n","    \n","    \n","You may find out about the dataset from the description given below."]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":false,"id":"oQqbRy42DUcz"},"outputs":[],"source":["from sklearn.datasets import load_breast_cancer\n","\n","cancer = load_breast_cancer()\n","\n","print(cancer.DESCR) # Print the data set description"]},{"cell_type":"markdown","metadata":{"id":"oBLEl9AODUcz"},"source":["The object returned by `load_breast_cancer()` is a scikit-learn Bunch object, which is similar to a dictionary.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0cCDqFwtDUcz"},"outputs":[],"source":["cancer.keys()"]},{"cell_type":"markdown","metadata":{"id":"Es9xlBuTDUcz"},"source":["### Question 0\n","\n","How many features does the breast cancer dataset have?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3p1s3vJTDUcz"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HBktbbzcDUcz"},"outputs":[],"source":["cancer.data"]},{"cell_type":"markdown","metadata":{"id":"16fgQsmzDUcz"},"source":["### Question 1\n","\n","Convert the sklearn.dataset `cancer` to a DataFrame."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lnut7V-KDUcz"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CL-YmdFIDUcz"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"O5KQiEGADUcz"},"source":["### Question 2\n","What is the class distribution? (i.e. how many instances of `malignant` and how many `benign` ?)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6D71E0xMDUcz"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"HlXopEYGDUcz"},"source":["### Question 3\n","Split the DataFrame into `X` (the data) and `y` (the labels)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"riEzOLIzDUc1"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"6_p8R6U3DUc1"},"source":["### Question 4\n","Using `train_test_split`, split `X` and `y` into training and test sets `(X_train, X_test, y_train, and y_test)`.\n","\n","Set the random number generator state to 0 using **`random_state = 0`**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4STOQ1BzDUc1"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"dDODUydaDUc1"},"source":["### Question 5\n","Using `LogisticRegression`, fit your model with `X_train`, `y_train`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"q-5QYNhFDUc1"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"gdeY7o49DUc1"},"source":["### Question 6\n","Using your logistic regression model, predict the class label."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mfFzBmcdDUc1"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"AQqIhX0BDUc1"},"source":["### Question 7\n","Predict the class labels for the test set `X_test`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SkqCooEXDUc1"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"l8RibgetDUc2"},"source":["### Question 8\n","Find the score (mean accuracy) of your logistic regression model using `X_test` and `y_test`.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kmBDbm9tDUc2"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"O2ddCi4aDUc2"},"source":["# 4. Support Vector Machine (SVM): Classification"]},{"cell_type":"markdown","metadata":{"id":"-ki5VJ2uDUc2"},"source":["Support vector machines (SVMs) are a particularly powerful and flexible class of supervised algorithms for both classification and regression. In this section, we will develop the intuition behind support vector machines and their use in classification problems."]},{"cell_type":"markdown","metadata":{"id":"yANgg5R9DUc2"},"source":["## Motivating SVMs\n","Given two possible categories for data points to fall into, we can approach the classification task in one of two ways:\n","- *Generative algorithms*, calculate the probability of a data point belonging to a certain class.\n","- *Discriminative algorithms*, on the other hand, identify the optimal boundary between classes.\n","\n","SVM is an example of a discriminative algorithm. While it may not seem as intuitively useful, it has two useful advantages over generative models.\n","\n","1. Discriminative models typically outperform generative models.\n","2. Discriminative models are more robust towards outliers than generative models.\n","\n","Consider the following toy data set."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WtpoWI9cDUc2"},"outputs":[],"source":["from mpl_toolkits.mplot3d import Axes3D\n","import sklearn.svm\n","import sklearn.datasets.samples_generator\n","import sklearn.model_selection\n","from scipy import stats\n","\n","\n","sns.set(style=\"dark\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-BczfdGdDUc2"},"outputs":[],"source":["from sklearn.datasets.samples_generator import make_blobs\n","\n","\n","X, y = make_blobs(\n","    n_samples=50, centers=2, random_state=0, cluster_std=0.60)\n","plt.scatter(X[:, 0], X[:, 1], c=np.array([\"red\", \"yellow\"])[y], s=50);"]},{"cell_type":"markdown","metadata":{"id":"6GZWDhDaDUc2"},"source":["Drawing a separating line between these two classes is easy eough, but which line should we choose?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UyHY68YADUc2"},"outputs":[],"source":["xfit = np.linspace(-1, 3.5)\n","plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')\n","plt.plot([0.6], [2.1], 'x', color='blue', markeredgewidth=2, markersize=10)\n","\n","for m, b in [(1, 0.65), (0.5, 1.6), (-0.2, 2.9)]:\n","    plt.plot(xfit, m * xfit + b, '-k')\n","\n","plt.xlim(-1, 3.5);"]},{"cell_type":"markdown","metadata":{"id":"N7MKj3bnDUc2"},"source":["All three of the above lines perfectly separate the training data. A new data point, denoted in blue above, would result in very different classification results. So how do we choose the separating line?"]},{"cell_type":"markdown","metadata":{"id":"NiANHRxzDUc2"},"source":["## Maximizing the margin:\n","An intuitive way to identify the \"best\" separating line is to draw a margin around the line. This margin begins at the line and extends in either direction until it touches a data point of that category."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zm5kaQQmDUc2"},"outputs":[],"source":["xfit = np.linspace(-1, 3.5)\n","plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')\n","\n","for m, b, d in [(1, 0.65, 0.33), (-0.2, 2.9, 0.2)]:\n","    yfit = m * xfit + b\n","    plt.plot(xfit, yfit, '-k')\n","    plt.fill_between(xfit, yfit - d, yfit + d, edgecolor='none',\n","                     color='#AAAAAA', alpha=0.4)\n","\n","plt.xlim(-1, 3.5);"]},{"cell_type":"markdown","metadata":{"id":"x0PKZ1ZNDUc2"},"source":["For the sake of simplicity, we'll only consider separating lines that lie in the middle of any given margin.\n","\n","So which margin is optimal? If our goal is to separate the two classes as well as possible, then the optimal margin will be the one with the largest width! This minimizes the probability of misclassification of a new data point."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3GbgKD6YDUc2"},"outputs":[],"source":["from sklearn.svm import SVC # \"Support vector classifier\"\n","\n","model = SVC(kernel=\"linear\").fit(X, y)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"w4YI3TimDUc3"},"outputs":[],"source":["def plot_svc_decision_function(model, ax=None, plot_support=True):\n","    \"\"\"Plot the decision function for a 2D SVC\"\"\"\n","    if ax is None:\n","        ax = plt.gca()\n","    xlim = ax.get_xlim()\n","    ylim = ax.get_ylim()\n","\n","    # create grid to evaluate model\n","    x = np.linspace(xlim[0], xlim[1], 30)\n","    y = np.linspace(ylim[0], ylim[1], 30)\n","    Y, X = np.meshgrid(y, x)\n","    xy = np.vstack([X.ravel(), Y.ravel()]).T\n","    P = model.decision_function(xy).reshape(X.shape)\n","\n","    # plot decision boundary and margins\n","    ax.contour(X, Y, P, colors='k',\n","               levels=[-1, 0, 1], alpha=0.5,\n","               linestyles=['--', '-', '--'])\n","\n","    # plot support vectors\n","    if plot_support:\n","        ax.scatter(model.support_vectors_[:, 0],\n","                   model.support_vectors_[:, 1],\n","                   s=300, linewidth=1, facecolors='none', color='black');\n","    ax.set_xlim(xlim)\n","    ax.set_ylim(ylim)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rtY_ySSTDUc4"},"outputs":[],"source":["plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')\n","plot_svc_decision_function(model);"]},{"cell_type":"markdown","metadata":{"id":"AzkKO3zxDUc4"},"source":["The highlighted circles are the **support vectors**. These are the core points that the model uses to determine the separating boundary. It's important to note that *only* these support vectors are important. All other data points are effectively disregarded during training!\n","\n","Below is an example of a toy dataset with additional data points outside of the previously defined margin."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nVeKpMqtDUc4"},"outputs":[],"source":["model.support_vectors_"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DcpgRtIzDUc4"},"outputs":[],"source":["def plot_svm(N=10, ax=None):\n","    X, y = make_blobs(n_samples=200, centers=2,\n","                      random_state=0, cluster_std=0.60)\n","    X = X[:N]\n","    y = y[:N]\n","    model = SVC(kernel='linear', C=1E10)\n","    model.fit(X, y)\n","    print('N = {0} \\n'.format(N), model.support_vectors_)\n","    print (\"_________________________________________\")\n","\n","    ax = ax or plt.gca()\n","    ax.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')\n","    ax.set_xlim(-1, 4)\n","    ax.set_ylim(-1, 6)\n","    plot_svc_decision_function(model, ax)\n","\n","fig, ax = plt.subplots(1, 2, figsize=(16, 6))\n","fig.subplots_adjust(left=0.0625, right=0.95, wspace=0.1)\n","for axi, N in zip(ax, [60, 120]):\n","    plot_svm(N, axi)\n","    axi.set_title('N = {0}'.format(N))"]},{"cell_type":"markdown","metadata":{"id":"MbH-uwLyDUc4"},"source":["We can confirm, visually and via the support vectors, that the two models are identical.\n","\n","**SVMs effectively ignore all data points that aren't support vectors during training**"]},{"cell_type":"markdown","metadata":{"id":"nQpCdVKdDUc4"},"source":["## Tuning the SVM: Softening the margin\n","What if outliers in the data affect the margin itself? Consider the data from before, but with an additional outlier inside the margin belonging to the red class (marked with an X)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3W534VsBDUc4"},"outputs":[],"source":["Xo = np.vstack((X, np.array([[0.5, 2.5]])))\n","yo = np.append(y, 0)\n","\n","plt.scatter(Xo[:, 0], Xo[:, 1], c=yo, s=50, cmap='autumn')\n","plt.scatter(0.5, 2.5, c='blue', s=50, cmap='autumn')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Mtgv5FShDUc4"},"outputs":[],"source":["models = [\n","    SVC(kernel=\"linear\").fit(X, y),\n","    SVC(kernel=\"linear\").fit(Xo, yo)]\n","\n","fig, ax = plt.subplots(1, 2, figsize=(16, 6))\n","fig.subplots_adjust(left=0.0625, right=0.95, wspace=0.1)\n","\n","for axi, model in zip(ax, models):\n","    ax[0].scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')\n","    plot_svc_decision_function(model, axi)\n","    axi.scatter(model.support_vectors_[:, 0],\n","                model.support_vectors_[:, 1],\n","                s=300, lw=1, facecolors='none')\n","    ax[1].scatter(Xo[:, 0], Xo[:, 1], c=yo, s=50, cmap='autumn')\n","    ax[1].scatter(0.5, 2.5, c=\"blue\", s=500, marker=\"x\")"]},{"cell_type":"markdown","metadata":{"id":"a0xgXuReDUc4"},"source":["Notice how a single outlier completely changes our decision boundary? This instability towards outliers close to the boundary is, obviously, an undesirable effect.\n","\n","To rectify this, we introduce a tuning parameter `C`, which determines the hardness of the boundary margin. for $C \\rightarrow \\infty$, the margin is completely hard and tolerates absolutely no overlaps with data points. As $C$ becomes smaller, the SVM tolerates further \"incursions\" into the margin.\n","\n","For more explanation about `C`, watch this video:\n","https://www.youtube.com/watch?v=5oVQBF_p6kY"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tjxl46BsDUc4"},"outputs":[],"source":["models = [\n","    SVC(kernel=\"linear\", C=1e10).fit(Xo, yo),\n","    SVC(kernel=\"linear\", C=1).fit(Xo, yo),\n","    SVC(kernel=\"linear\", C=0.5).fit(Xo, yo),\n","    SVC(kernel=\"linear\", C=0.25).fit(Xo, yo)]\n","\n","fig, ax = plt.subplots(2, 2, figsize=(12, 12))\n","fig.subplots_adjust(left=0.0625, right=0.95, wspace=0.1)\n","ax[0][0].scatter(Xo[:, 0], Xo[:, 1], c=yo, s=50, cmap='autumn');\n","ax[0][1].scatter(Xo[:, 0], Xo[:, 1], c=yo, s=50, cmap='autumn');\n","ax[1][0].scatter(Xo[:, 0], Xo[:, 1], c=yo, s=50, cmap='autumn');\n","ax[1][1].scatter(Xo[:, 0], Xo[:, 1], c=yo, s=50, cmap='autumn');\n","plot_svc_decision_function(models[0], ax=ax[0][0]);\n","plot_svc_decision_function(models[1], ax=ax[0][1]);\n","plot_svc_decision_function(models[2], ax=ax[1][0]);\n","plot_svc_decision_function(models[3], ax=ax[1][1]);\n","\n","ax[0][0].scatter(0.5, 2.5, c=\"red\", s=500, marker=\"x\");\n","ax[0][1].scatter(0.5, 2.5, c=\"red\", s=500, marker=\"x\");\n","ax[1][0].scatter(0.5, 2.5, c=\"red\", s=500, marker=\"x\");\n","ax[1][1].scatter(0.5, 2.5, c=\"red\", s=500, marker=\"x\");"]},{"cell_type":"markdown","metadata":{"id":"fFcrkqH8DUc4"},"source":["We can't exactly recreate the original separating boundary but can get close.\n","\n","This softening of the boundary isn't just useful for dealing with individual outliers but also for when the data simply overlaps too strongly. For example:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rVCpT9S-DUc4"},"outputs":[],"source":["X, y = make_blobs(n_samples=100, centers=2, random_state=0, cluster_std=1.2)\n","model = SVC(kernel=\"linear\", C=1).fit(X, y)\n","plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn');\n","plot_svc_decision_function( model)"]},{"cell_type":"markdown","metadata":{"id":"iqOf4I9eDUc5"},"source":["Fun fact: Trying to train a model with a very high value for $C$ will take an exceptionally long time or even fail outright due to the algorithms inability to find an acceptable solution without overlaps."]},{"cell_type":"markdown","metadata":{"id":"qeKG2C-VDUc5"},"source":["### Non-linear kernels\n","A linear separating boundary is only one option we have for an SVM. Just as we can project features into higher-dimensional spaces for linear regressions, e.g. polynomial features, we can also project features into a higher-dimensional space for SVMs.\n","\n","Scikit-learn lets us directly define the feature transformation in the model. This is because SVMs are special in that we don't explicitly need to calculate the transformed features; we can use mathematical tricks to do so implicitly.\n","\n","Several types of kernels are available by default."]},{"cell_type":"markdown","metadata":{"id":"ZeSIsHWDDUc5"},"source":["### Gaussian Radial Basis Function\n","The most common, and most powerful, kernel is the **Gaussian radial basis function (rbf)**. Let us consider the following data:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ju-yRzgKDUc5"},"outputs":[],"source":["from sklearn.datasets.samples_generator import make_circles\n","\n","X, y = make_circles(500, factor=0.2, noise=0.2, random_state=0)\n","\n","plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn');\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UzxZMi2rDUc5"},"outputs":[],"source":["model = SVC(kernel='linear').fit(X, y)\n","plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn');\n","plot_svc_decision_function(model, plot_support=False);\n","print(\"Model Accuracy: {:.2f}\".format(model.score(X, y)))"]},{"cell_type":"markdown","metadata":{"id":"owHJSGIjDUc5"},"source":["There is no way we could possible separate this data linearly, no mtter how soft the margin is. We can, however, lift it into a third dimension with a radial basis function:\n","\n","$$ Z(x) = e^{-\\gamma \\cdot \\sum_i x_i^2} $$"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2ig2geT9DUc5"},"outputs":[],"source":["gamma = 1\n","z = np.exp(-gamma*(X ** 2).sum(1))\n","fig = plt.figure()\n","ax = fig.add_subplot(111, projection=\"3d\")\n","ax.scatter3D(X[:, 0], X[:, 1], z, c=np.array([\"red\", \"yellow\"])[y], s=50)\n","ax.view_init(elev=30, azim=30)\n","ax.set_xlabel('x')\n","ax.set_ylabel('y')\n","ax.set_zlabel('r')"]},{"cell_type":"markdown","metadata":{"id":"72re5HKUDUc6"},"source":["As we can see, this adds a third dimension to our data. We can now cleanly separate the data points by cutting the resulting cone."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o1uCkuWsDUc6"},"outputs":[],"source":["model = SVC(kernel='rbf', gamma=\"scale\").fit(X, y)\n","plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn');\n","plot_svc_decision_function(model, plot_support=False);\n","print(\"Model Accuracy: {:.2f}\".format(model.score(X, y)))"]},{"cell_type":"markdown","metadata":{"id":"UedhN7W6DUc6"},"source":["Intuitively, the `gamma` parameter defines how far the influence of a single training example reaches, with low values meaning ‘far’ and high values meaning ‘close’. The gamma parameters can be seen as the inverse of the radius of influence of samples selected by the model as support vectors.\n","\n","For more explanation regarding `gamma` watch this video:\n","https://www.youtube.com/watch?v=m2a2K4lprQw"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Qe3No0B-DUc6"},"outputs":[],"source":["model = sklearn.svm.SVC(kernel='rbf', gamma=50).fit(X, y)\n","plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn');\n","plot_svc_decision_function(model, plot_support=False);\n","print(\"Model Accuracy: {:.2f}\".format(model.score(X, y)))"]},{"cell_type":"markdown","metadata":{"id":"CM1_DyNyDUc6"},"source":["### Other kernels\n","In most cases, the RBF kernel will be the kernel of choice. However, SVMs also natively support polynomial, sigmoidal, or even custom kernels. See the [Scikit-learn documentation for SVM Classifiers](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html) for more information."]},{"cell_type":"markdown","metadata":{"id":"WzaGMDp9DUc6"},"source":["**Exercise**\n","\n","Given the data below, train support vector machines with various kernels and parameters to study the effects thereof.\n","\n","You can visualize all of your results with the command (the model must be trained beforehand!)\n","\n","```python\n","plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')\n","plot_svc_decision_function(model, plot_support=False);\n","```"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v0Ns25Q4DUc6"},"outputs":[],"source":["rng = np.random.RandomState(0)\n","X = rng.normal(size=(250, 2))\n","y = (X[:, 0] / X[:, 1] > 0).astype(int)\n","X += rng.normal(scale=0.2, size=(250, 2))\n","plt.scatter(X[:, 0], X[:, 1], c=y, cmap=\"autumn\")"]},{"cell_type":"markdown","metadata":{"id":"fE7FKbroDUc7"},"source":["1. Separate the data into a training and a test data set. Let the test set size fraction be 25%.\n","2. Train an SVM with a linear kernel with the default value for `C` and assess the performance on the test set. Plot the results.\n","3. Now train a model with an `rbf` kernel and assess its performance. Set `C=1` and `gamma=\"auto\"`. Plot the results.\n","4. Train a model with `gamma` values of `(0.01, 0.1, 1, 10, 100)`. Assess the performance of each model and plot the results. You'll need to place each plot into a separate notebook cell due to how the `plot_svc_decision_function(...)` function is implemented.\n","5. Train an SVM with a `poly` kernel, `degree=2`, `gamma=100`, and `C=1`. Assess the performance and visualize the results. How does this result compare to the `rbf` kernel above?"]},{"cell_type":"markdown","metadata":{"id":"rKV4j4ppDUc7"},"source":["- Separate the data into a training and a test data set. Let the test set size fraction be 25%."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5dwCLs60DUc7"},"outputs":[],"source":["### Your code here"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"l_CJrkIMDUc7"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"946AkvF7DUc7"},"source":["- Train an SVM with a linear kernel with the default value for `C` and assess the performance on the test set. Plot the results."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FaGvrhCrDUc7"},"outputs":[],"source":["### Your code here"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bwIeYFjCDUc7"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"FWWckAZxDUc8"},"source":["- Now train a model with an `rbf` kernel and assess its performance. Set `C=1` and `gamma=\"auto\"`. Plot the results."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d1Dxqa3cDUc8"},"outputs":[],"source":["### Your code here"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jWF9XNDVDUc8"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"5v5ajU41DUc8"},"source":["- Train a model with `gamma` values of `(0.01, 0.1, 1, 10, 100)`. Assess the performance of each model and plot the results. You'll need to place each plot into a separate notebook cell due to how the `plot_svc_decision_function(...)` function is implemented."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xLuu1kWGDUc8"},"outputs":[],"source":["### Your code here"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a3tnne8cDUc8"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Kt-UkRXSDUc8"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f4rQobVkDUc8"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j6UCPyjHDUc8"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sY8dxJPLDUc9"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"P2eochmKDUc9"},"source":["- Train an SVM with a `poly` kernel, `degree=2`, `gamma=100`, and `C=1`. Assess the performance and visualize the results. How does this result compare to the `rbf` kernel above?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LDwyTjxjDUc9"},"outputs":[],"source":["### Your code here"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_3f3eQYcDUc9"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"n2owh3awDUc9"},"source":["## More Readings:\n","\n","Distance:\n","\n","- http://www.cs.ucc.ie/~dgb/courses/tai/notes/handout4.pdf\n","\n","Decision Tree:\n","\n","- https://medium.com/machine-learning-101/chapter-3-decision-trees-theory-e7398adac567\n","\n","Logistic Regression:\n","\n","- https://www.coursera.org/lecture/machine-learning/classification-wlPeP\n","\n","- https://www.coursera.org/lecture/machine-learning/decision-boundary-WuL1H\n","\n","- https://www.coursera.org/lecture/ml-classification/intuition-behind-linear-classifiers-lCBwS\n","\n","- https://www.youtube.com/watch?v=l-EhAIp31HA\n","\n","- https://www.youtube.com/watch?v=Q81RR3yKn30\n","\n","- https://www.coursera.org/lecture/machine-learning/regularized-logistic-regression-4BHEy\n","\n","Support Vector Machine (SVM):\n","\n","- Bacis Friendly Introduction: https://www.youtube.com/watch?v=N1vOgolbjSc\n","\n","- Bacis Friendly Introduction: https://www.youtube.com/watch?v=Y6RRHw9uN9o\n","\n","- Kernels (Advanced): https://www.coursera.org/lecture/machine-learning/kernels-i-YOMHn\n","\n","- Set `C`: https://www.youtube.com/watch?v=5oVQBF_p6kY\n","\n","- Set `gamma`: https://www.youtube.com/watch?v=m2a2K4lprQw"]}],"metadata":{"anaconda-cloud":{},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.3"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}